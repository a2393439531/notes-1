# 中文分词器介绍

随着语义网的发展，智能语义分析和知识图谱正应用在智能检索及人机交流上，文本识别正是其中最基础的技术。而分词系统又是文本识别的基础，足见其重要性。而中文在基本文法上有其特殊性，比如词和词组边界模糊，古汉语的词通常是单字等，导致其比英文等其他语言分词更加复杂。

比如用户搜索“习近平的生日”这句话，系统必须首先弄明白用户想要什么，最开始系统会解析这句话，这么处理：“习近平/nr（人名） 的/ude1（助词） 生日/n（名词）”，根据词性标注，系统会知道这是在找某个人的什么东西，然后会根据语义网内容去检索这个人的“生日”，并推荐出最高契合的那个。这个最开始的处理用到的就是分词的功能。

近几年随着大数据和互联网的发展，中文分词器如雨后春笋般的涌现出来，一些商用的比如东北大学的NiuParser和中科院的ICTCLAS都做的非常好，功能强大，准确率高，速度快。

现在的中文分词器在经历了词典词库查询、人名地名识别、歧义识别后现正走向自动学习、自动发现新词、具有句法分析等功能。现就项目中使用过的几个开源的分词器给大家介绍一下。

## Ansj分词器

Ansj是一个基于google语义模型加条件随机场模型的基于中科院ICTCLAS中文分词的java实现，由于项目使用lucene + solr，所以Ansj也是我们在项目中最终选用的分词器，其特点如下：

- 算法：基于google语义模型 + 条件随机场模型的中文分词；
- 分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上；
- 目前实现了 .中文分词 .中文姓名识别 .用户自定义词典（包括歧义字典）；
- 可以应用到自然语言处理等方面，适用于对分词效果要求高的各种项目；
- 分词模式：分为 BasicAnalyzer（词典模式）、ToAnalyzer（精准分词，普通模式）、NlpAnalyzer（自然语言分词，最准确）、IndexAnalyzer（索引模式分词，信息覆盖最广）；
- 关键字提取；
- 拼音转换；
- 词性标注（基于中科院ICTCLAS）；
- 简单句法分析；
- 新词发现功能；
- lucene+solr友好（官方提供相应插件）；
- 持续维护中（有些分词器已经停止维护了，比如大家熟知的IKAnalyzer、庖丁等分词器）；

官方网址：http://www.nlpcn.org/

使用帮助：http://nlpchina.github.io/ansj_seg/

## 结巴分词器

项目中需要制作大量语义原型，而Python脚本语言在制作原型上堪称神器，所以我就像找一款Python的分词器，于是就发现了“结巴”分词器（名字很屌丝，但用起来很高快广），以下是其特点：

- 算法：基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；
- 分词模式：分为全模式（信息覆盖广，但有可能放大不必要的信息）；精确模式（通用模式）；新词识别模式；搜索引擎模式（信息覆盖广，但有可能放大不必要的信息）；
- 关键词提取（基于TextRank算法）；
- 词性标注（基于中科院ICTCLAS）；
- 并行分词（可指定多个进程同时分词，速度快，实测可达1.5Mb/s）；
- Tokenize：可返回词语在原文中位置；
- 支持命令行分词（Python版）；
- 自定义词典支持（包括歧义字典）；
- 采用字典延迟加载机制；
- 新词发现功能（基于HMM模型的Viterbi算法）；
- Solr友好（官方提供相应插件）；
- 多语言支持（目前支持Python、Java、C++、Node.js、Erlang、R、iOS）
- 持续维护中；

项目网址：https://github.com/fxsjy/jieba/

在线Demo：http://jiebademo.ap01.aws.af.cm/

## 其他分词器

其他如IKAnalyzer、Paoding等分词器，因为都已不再维护，在此就不多说了，感兴趣的话，大家自己查看一下就行。
